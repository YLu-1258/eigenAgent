# eigenAgent ğŸ§ âš¡

**eigenAgent** is a fully local, desktop AI assistant built with **Rust**, **Tauri**, **React**, and **llama.cpp**.  
It runs quantized GGUF models on-device, streams responses token-by-token, renders Markdown + LaTeX, and cleanly separates *model reasoning* from *final answers*.

No cloud. No API keys. Your data stays on your machine.

![Example window of eigenAgent](sample_window.png)
---

## âœ¨ Features

- ğŸ–¥ï¸ **Local-first AI** using GGUF models (llama.cpp)
- âš¡ **Token-level streaming** responses
- ğŸ§  **Hidden chain-of-thought** (`<think>...</think>`) with optional side-panel viewing
- âœï¸ **Markdown rendering** (GitHub-flavored)
- ğŸ“ **LaTeX math rendering** via KaTeX
- ğŸ’¬ **Chat history UI** with assistant / user roles
- ğŸªŸ **Desktop app** via Tauri (macOS / Windows / Linux)
- ğŸ¨ **Custom dark UI** with styled scrollbars
- ğŸš€ **Non-blocking model load** with startup loading screen

---

## ğŸ§± Tech Stack

**Backend**
- Rust
- Tauri
- [`llama_cpp_2`](https://crates.io/crates/llama_cpp_2)
- GGUF quantized models (e.g. Qwen, LLaMA)

**Frontend**
- React + TypeScript
- `react-markdown`
- `remark-gfm`
- `remark-math` + `rehype-katex`
- Custom CSS (no UI framework)

---

## ğŸ“‚ Project Structure

```
eigenAgent/
â”œâ”€â”€ src-tauri/
â”‚ â””â”€â”€ src/
â”‚ â””â”€â”€ lib.rs # Rust backend (model loading, streaming, state)
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ App.tsx # React UI
â”‚ â”œâ”€â”€ App.css # UI + scrollbar styling
â”‚ â””â”€â”€ main.tsx
â”œâ”€â”€ models/
â”‚ â””â”€â”€ *.gguf # Local LLM models (not committed)
â””â”€â”€ README.md
```
---

## ğŸ§  How It Works

1. **Startup**
   - `LlamaBackend` initializes
   - Model loads asynchronously (non-blocking)
   - Frontend listens for `model:ready`

2. **Chat**
   - Frontend sends user prompt (and optional history)
   - Backend formats ChatML-style prompt
   - Tokens are streamed back via Tauri events:
     - `chat:begin`
     - `chat:delta`
     - `chat:end`

3. **Reasoning Control**
   - Model is instructed to wrap internal reasoning in `<think>...</think>`
   - Frontend:
     - hides `<think>` content from the main chat
     - optionally displays it in a side panel

---

## ğŸ§ª Supported Models

Any **GGUF** model compatible with llama.cpp should work.

Recommended:
- Qwen / Qwen3
- LLaMA-family models
- Mistral-family models

Example:
```
Qwen3VL-4B-Thinking-Q4_K_M.gguf
```
> âš ï¸ Models are **not** included in the repo.

---

## ğŸš€ Getting Started

### Prerequisites
- Rust (stable)
- Node.js (18+)
- pnpm / npm / yarn
- Tauri CLI

```bash
cargo install tauri-cli
```

### Install Dependencies
```bash
npm install
```

### Add a model
```
models/
â””â”€â”€ your-model.gguf
```
Update the model path in:
```
src-tauri/src/lib.rs
```

### Run the app
```
npm run tauri dev
```